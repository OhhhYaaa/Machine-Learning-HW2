{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "0cc7d476-5c6f-4e07-934a-3127f4b85fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(120000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 120 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import xgboost\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import ensemble, preprocessing, metrics\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "import category_encoders as ce\n",
    "import lightgbm as lgb\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from random import shuffle\n",
    "from sklearn.model_selection import KFold\n",
    "%autosave 120"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "b26a0f8b-e592-4f47-9d49-7cdc10daed58",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39063, 2)\n",
      "(58592, 43)\n",
      "(39062, 42)\n",
      "   policy_tenure  age_of_car  age_of_policyholder  area_cluster  \\\n",
      "0              0           0                    0             0   \n",
      "\n",
      "   population_density  make  segment  model  fuel_type  max_torque  ...  \\\n",
      "0                4990     1        0      0          0           5  ...   \n",
      "\n",
      "   is_brake_assist  is_power_door_locks  is_central_locking  \\\n",
      "0                0                    0                   0   \n",
      "\n",
      "   is_power_steering  is_driver_seat_height_adjustable  \\\n",
      "0                  1                                 0   \n",
      "\n",
      "   is_day_night_rear_view_mirror  is_ecw  is_speed_alert  ncap_rating  \\\n",
      "0                              0       0               1            0   \n",
      "\n",
      "   is_claim  \n",
      "0         0  \n",
      "\n",
      "[1 rows x 43 columns]\n",
      "['policy_tenure', 'age_of_car', 'age_of_policyholder', 'area_cluster', 'population_density']\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 58592 entries, 0 to 58591\n",
      "Data columns (total 43 columns):\n",
      " #   Column                            Non-Null Count  Dtype\n",
      "---  ------                            --------------  -----\n",
      " 0   policy_tenure                     58592 non-null  int64\n",
      " 1   age_of_car                        58592 non-null  int64\n",
      " 2   age_of_policyholder               58592 non-null  int64\n",
      " 3   area_cluster                      58592 non-null  int64\n",
      " 4   population_density                58592 non-null  int64\n",
      " 5   make                              58592 non-null  int64\n",
      " 6   segment                           58592 non-null  int64\n",
      " 7   model                             58592 non-null  int64\n",
      " 8   fuel_type                         58592 non-null  int64\n",
      " 9   max_torque                        58592 non-null  int64\n",
      " 10  max_power                         58592 non-null  int64\n",
      " 11  engine_type                       58592 non-null  int64\n",
      " 12  airbags                           58592 non-null  int64\n",
      " 13  is_esc                            58592 non-null  int64\n",
      " 14  is_adjustable_steering            58592 non-null  int64\n",
      " 15  is_tpms                           58592 non-null  int64\n",
      " 16  is_parking_sensors                58592 non-null  int64\n",
      " 17  is_parking_camera                 58592 non-null  int64\n",
      " 18  rear_brakes_type                  58592 non-null  int64\n",
      " 19  displacement                      58592 non-null  int64\n",
      " 20  cylinder                          58592 non-null  int64\n",
      " 21  transmission_type                 58592 non-null  int64\n",
      " 22  gear_box                          58592 non-null  int64\n",
      " 23  steering_type                     58592 non-null  int64\n",
      " 24  turning_radius                    58592 non-null  int64\n",
      " 25  length                            58592 non-null  int64\n",
      " 26  width                             58592 non-null  int64\n",
      " 27  height                            58592 non-null  int64\n",
      " 28  gross_weight                      58592 non-null  int64\n",
      " 29  is_front_fog_lights               58592 non-null  int64\n",
      " 30  is_rear_window_wiper              58592 non-null  int64\n",
      " 31  is_rear_window_washer             58592 non-null  int64\n",
      " 32  is_rear_window_defogger           58592 non-null  int64\n",
      " 33  is_brake_assist                   58592 non-null  int64\n",
      " 34  is_power_door_locks               58592 non-null  int64\n",
      " 35  is_central_locking                58592 non-null  int64\n",
      " 36  is_power_steering                 58592 non-null  int64\n",
      " 37  is_driver_seat_height_adjustable  58592 non-null  int64\n",
      " 38  is_day_night_rear_view_mirror     58592 non-null  int64\n",
      " 39  is_ecw                            58592 non-null  int64\n",
      " 40  is_speed_alert                    58592 non-null  int64\n",
      " 41  ncap_rating                       58592 non-null  int64\n",
      " 42  is_claim                          58592 non-null  int64\n",
      "dtypes: int64(43)\n",
      "memory usage: 19.7 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "sample_submission = pd.read_csv('./sample_submission.csv')\n",
    "train = pd.read_csv('./train.csv')\n",
    "test = pd.read_csv('./test.csv')\n",
    "t = train['is_claim']\n",
    "train = train.drop(['is_claim'], axis = 1)\n",
    "tmp = pd.concat([train, test], axis = 0)\n",
    "tmp = tmp.drop(['policy_id'], axis = 1)\n",
    "labelencoder = LabelEncoder()\n",
    "for i in tmp.columns[:-1] : \n",
    "    if(tmp[i].dtype == object):\n",
    "        tmp[i] = labelencoder.fit_transform(tmp[i])\n",
    "tmp['turning_radius'] = tmp['turning_radius'] * 100\n",
    "tmp['turning_radius'] = tmp['turning_radius'].astype(int)\n",
    "\n",
    "# 連續變數離散化\n",
    "tmp1 = tmp.copy()\n",
    "a = tmp1.columns\n",
    "b = 22\n",
    "c = []\n",
    "    \n",
    "for i in range(tmp1.shape[1]) :\n",
    "    if(tmp1[a[i]].dtype != object) :\n",
    "        if(len(tmp1[a[i]].unique()) >= b) :\n",
    "            tmp1[a[i]] = pd.cut(tmp1[a[i]], b)\n",
    "            tmp1[a[i]] = tmp[a[i]].astype(int)\n",
    "            c.append(a[i])\n",
    "\n",
    "train1 = tmp1.iloc[:train.shape[0], ::]\n",
    "train1 = pd.concat([train1, t], axis = 1)\n",
    "test1 = tmp1.iloc[train.shape[0]+1:, ::]\n",
    "print(sample_submission.shape)\n",
    "print(train1.shape)\n",
    "# print(train.columns)\n",
    "print(test1.shape)\n",
    "print(train1.head(1))\n",
    "print(c)\n",
    "print(train1.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7841646f-7630-41d4-926b-5de8b98be770",
   "metadata": {},
   "source": [
    "## Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3573f3ec-410f-42e2-ab0f-252ae1830d7b",
   "metadata": {},
   "source": [
    "### Bayes method1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "id": "b474e54e-0f0e-4ccd-801b-be27fd0491eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score(y, y_pred) : \n",
    "    r = sum(y == y_pred)/len(y)\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "37b18ec4-ca78-4592-b507-60737b512b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calcualting probabilites for inputs independantly  p(A)\n",
    "def prior(data, col):\n",
    "    t = data[col]\n",
    "    t = t.value_counts()\n",
    "    return t/data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "2f65a95c-a571-4593-9d8a-f722133f438d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#calculating conditional probability  p(A|B)\n",
    "def posterior(data, want, given):\n",
    "    ab = data[[want, given]]\n",
    "    a = data[want].unique()\n",
    "    b = data[given].unique()\n",
    "    \n",
    "    groups = ab.groupby(by = [given, want]).size().reset_index()\n",
    "    a = pd.DataFrame(data[given].value_counts()[groups[given]]).reset_index()\n",
    "    groups = pd.concat([groups, a], axis = 1)\n",
    "    groups.columns = ['B', 'A', 'P(A|B)', 'i', 'B_total']\n",
    "    groups['P(A|B)'] = groups['P(A|B)']/groups.B_total\n",
    "    groups = groups.drop(['i', 'B_total'], axis = 1)\n",
    "\n",
    "    return  groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "id": "8d352f0d-262d-4949-b6f3-eef5569f557e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Naive_Bayes(x_train, x_test, out):\n",
    "    ans = []\n",
    "    y_train = x_train[out]\n",
    "    y_prob = prior(x_train, out)\n",
    "    y = x_train[out].unique()\n",
    "    test_ans = x_test[out]\n",
    "    test = x_test.drop([out], axis = 1)\n",
    "    for s in range(len(test)) :\n",
    "        t = test.iloc[s, ::]\n",
    "        for i in y :\n",
    "            each = []\n",
    "            e = y_prob[y]\n",
    "            for j in x_train.columns[:-1] :\n",
    "                tmp = posterior(x_train, j, out)\n",
    "                idx1 = tmp['A'] == t[j]\n",
    "                idx2 = tmp['B'] == i\n",
    "                tmp = tmp[idx1 & idx2]\n",
    "                if (tmp.empty):\n",
    "                    tmp = 0\n",
    "                else : \n",
    "                    tmp = tmp.iloc[0,2] \n",
    "                e = e * tmp\n",
    "            each.append(e)\n",
    "        a = np.argmax(e)\n",
    "        ans.append(y[a])\n",
    "    acc = sum( test_ans == ans )\n",
    "    \n",
    "    return acc, ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a3dc9e7a-cca1-4cc1-b5e8-925a1044cd1e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "973.5264370441437\n"
     ]
    }
   ],
   "source": [
    "# 9hr\n",
    "s = time.time()\n",
    "train2 = train1.iloc[:3000, ::]\n",
    "a = round(len(train2) * 0.7)\n",
    "w = random.sample(range(len(train2)), a)\n",
    "b = list(range(len(train2)))\n",
    "w1 = [a - b for a, b in zip( list(range(len(train2))), w)]\n",
    "tr = train2.iloc[w]\n",
    "te = train2.iloc[w1]\n",
    "acc, ans = Naive_Bayes(tr, te, 'is_claim')\n",
    "print(time.time() - s)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99ead78-923e-4553-b520-41e47f13484f",
   "metadata": {},
   "source": [
    "### Bayes method2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "id": "c34adc0a-90b5-4b9a-84f8-1d60e91a29f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Naive_Bayes :\n",
    "    def __init__(self) :\n",
    "        self.feats = list\n",
    "        self.likelihoods = {}\n",
    "        self.class_priors = {}\n",
    "        self.pred_priors = {}\n",
    "        self.x = np.array\n",
    "        self.y = np.array\n",
    "        self.nsample = int\n",
    "        self.nfeats = int\n",
    "\n",
    "    def fit(self, x, y) :\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.feats = list(x.columns)\n",
    "        self.nsample = x.shape[0]\n",
    "        self.nfeats = x.shape[1]\n",
    "\n",
    "        for i in self.feats :\n",
    "            self.likelihoods[i] = {}\n",
    "            self.pred_priors[i] = {}\n",
    "            for j in np.unique(self.x[i]) :\n",
    "                self.pred_priors[i].update({j : 0})\n",
    "                for k in np.unique(self.y) :\n",
    "                    self.likelihoods[i].update({str(j) + '_' + str(k) : 0})\n",
    "                    self.class_priors.update({k : 0})\n",
    "                \n",
    "        self.cal_class_prior()\n",
    "        self.cal_likelihoods()\n",
    "        self.cal_predictor_prior()\n",
    "        \n",
    "    def cal_class_prior(self) : # P(A)\n",
    "        for i in np.unique(self.y) :\n",
    "            self.class_priors[i] = sum(self.y == i)/self.nsample\n",
    "    \n",
    "    def cal_likelihoods(self) : # P(A|B)\n",
    "         for feature in self.feats:\n",
    "            for outcome in np.unique(self.y):\n",
    "                outcome_count = sum(self.y == outcome)\n",
    "                feat_likelihood = self.x[feature][self.y[self.y == outcome].index.values.tolist()].value_counts().to_dict()\n",
    "\n",
    "                for feat_val, count in feat_likelihood.items():\n",
    "                    if (outcome_count != 0) : \n",
    "                        self.likelihoods[feature][str(feat_val) + '_' + str(outcome)] = count/outcome_count\n",
    "                    else : \n",
    "                        self.likelihoods[feature][str(feat_val) + '_' + str(outcome)] = 0\n",
    "        \n",
    "    def cal_predictor_prior(self):\n",
    "        for feature in self.feats:\n",
    "            feat_vals = self.x[feature].value_counts().to_dict()\n",
    "\n",
    "            for feat_val, count in feat_vals.items():\n",
    "                self.pred_priors[feature][feat_val] = count/self.nsample   \n",
    "        \n",
    "        \n",
    "    def predict(self, x) :\n",
    "        result = []\n",
    "        x = np.array(x)\n",
    "        for query in x : \n",
    "            probs_outcome = {}\n",
    "            for out in np.unique(self.y) :\n",
    "                prior = self.class_priors[out]\n",
    "                likelihood = 1\n",
    "                e = 1\n",
    "                for i, j in zip(self.feats, query) :\n",
    "                    t = str(j) + '_' + str(out)\n",
    "                    likelihood *= self.likelihoods[i][t]\n",
    "                    e *= self.pred_priors[i][j]\n",
    "                if (e != 0) : \n",
    "                    posterior = (likelihood * prior) / e\n",
    "                else : \n",
    "                    posterior = 0\n",
    "                probs_outcome[out] = posterior\n",
    "            r = max(probs_outcome, key = lambda a : probs_outcome[a])\n",
    "            result.append(r)\n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "dabf33d0-2332-489a-9ab3-e06203643985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9325352318720437\n",
      "[0 0 0 ... 0 0 0]\n",
      "107\n",
      "29.863337516784668\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "a = round(len(train1) * 0.7)\n",
    "random.seed(1)\n",
    "w = random.sample(range(len(train1)), a)\n",
    "b = list(range(len(train1)))\n",
    "w1 = [a - b for a, b in zip( list(range(len(train1))), w)]\n",
    "tr = train1.iloc[w, ::]\n",
    "te = train1.iloc[w1, ::]\n",
    "\n",
    "a = Naive_Bayes()\n",
    "a.fit(tr.iloc[::, :-1], tr.iloc[::, -1])\n",
    "p = a.predict(te.iloc[::, :-1])\n",
    "acc = accuracy_score(p, te.iloc[::, -1]) \n",
    "\n",
    "print(acc)\n",
    "print(p)\n",
    "print(sum(p))\n",
    "print(time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed35fa07-8b51-4275-ae21-eee40c3155f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0f03c81-ac6a-4fa3-a4cf-fcd1a40b241d",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "id": "05451c21-1a37-441a-972b-defe030ff787",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'        \\n        \\n    # 3.3 Accuracy\\n    def calculate_accuracy(self, df, tree):\\n        predictions = make_predictions(df, tree)\\n        predictions_correct = predictions == df.label\\n        accuracy = predictions_correct.mean()\\n\\n        return accuracy\\n'"
      ]
     },
     "execution_count": 373,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self\n",
    "'''\n",
    "Note that you can implement the simplifiedversion of the random forest\n",
    "the full function is unnecessary in this assignment. \n",
    "Basically, you are required to implement the bagging in #data and feature dimension with the predefined ratio. \n",
    "The other internal hyperparameters of the #tree, #max-depth of each tree, etc., could be predefined by\n",
    "yourself, just mentioned in your report.\n",
    "\n",
    "1. bagging in #data\n",
    "2. internal hyperparameters of the #tree, #max-depth of each tree\n",
    "\n",
    "'''\n",
    "\n",
    "class RandomForestClassifier :\n",
    "    def __init__(self) -> None:\n",
    "        pass\n",
    "\n",
    "    \n",
    "    def train_test_split(self, data, test) : \n",
    "        a = data.shape[0]\n",
    "        train_size = round(a*test)\n",
    "        train_index = random.sample(range(a), b) \n",
    "        test_index = [a - b for a, b in zip( list(range(len(data))), w)]\n",
    "        train_df = data.iloc[train_index, ::]\n",
    "        test_df = data.iloc[test_index, ::]\n",
    "        \n",
    "        return train_df, test_df\n",
    "    \n",
    "    # 1. Decision Tree helper functions\n",
    "    # 1.1 Data pure?\n",
    "    def check_purity(self, data) : # 確認資料是否不用再被切分\n",
    "        y = data[:, -1]\n",
    "        unique_classes = np.unique(y)\n",
    "        if len(unique_classes) == 1 :\n",
    "            return True\n",
    "        else : \n",
    "            return False\n",
    "    \n",
    "    # 1.2 Classify    \n",
    "    def create_leaf(self, data) : # 找最大的類別\n",
    "        label_column = data[:, -1]\n",
    "        unique_classes, counts_unique_classes = np.unique(label_column, return_counts = True)\n",
    "        index = counts_unique_classes.argmax()\n",
    "        leaf = unique_classes[index]\n",
    "        \n",
    "        return leaf\n",
    "    \n",
    "    # 1.3 Determine potential splits\n",
    "    def get_potential_split(self, data) : # 列出所有出現過的變數種類\n",
    "        potential_splits = {}\n",
    "        n_columns = data.shape[1]\n",
    "        for i in range(n_columns - 1) :\n",
    "            values = data[::, i]\n",
    "            unique_values = np.unique(values)\n",
    "            potential_splits[i] = unique_values\n",
    "            \n",
    "        return potential_splits\n",
    "    \n",
    "    # 1.4 Determine Best Split\n",
    "    def cal_entropy(self, data) : # 計算cross entropy\n",
    "        label_column = data[:, -1]\n",
    "        counts = np.unique(label_column, return_counts=True)[1]\n",
    "        probabilities = counts / counts.sum()\n",
    "        e = sum(probabilities * -np.log2(probabilities))\n",
    "\n",
    "        return e\n",
    "    \n",
    "    def cal_overall_metrix(self, data_below, data_above) : # 看該次分組的成效?\n",
    "        n = len(data_below) + len(data_above)\n",
    "        p_data_below = len(data_below) / n\n",
    "        p_data_above = len(data_above) / n\n",
    "\n",
    "        overall_metric =  (p_data_below * self.cal_entropy(data_below) + p_data_above * self.cal_entropy(data_above))\n",
    "\n",
    "        return overall_metric\n",
    "    \n",
    "    def determine_best_split(self, data, potential_splits) : # 找 要切的變數 跟 要切的值\n",
    "        best_overall_metric = np.inf # 自己加的     \n",
    "        for column_index in potential_splits:\n",
    "            for value in potential_splits[column_index]:\n",
    "                data_below, data_above = self.split_data(data, split_column=column_index, split_value=value)\n",
    "                current_overall_metric = self.cal_overall_metrix(data_below, data_above)\n",
    "                if current_overall_metric <= best_overall_metric:\n",
    "                    best_overall_metric = current_overall_metric\n",
    "                    best_split_column = column_index\n",
    "                    best_split_value = value\n",
    "\n",
    "        return best_split_column, best_split_value\n",
    "\n",
    "    # 1.5 Split data\n",
    "    def split_data(self, data, split_column, split_value): # 切資料\n",
    "\n",
    "        split_column_values = data[:, split_column]\n",
    "\n",
    "        type_of_feature = FEATURE_TYPES[split_column]\n",
    "        if type_of_feature == \"continuous\":\n",
    "            data_below = data[split_column_values <= split_value]\n",
    "            data_above = data[split_column_values >  split_value]\n",
    "\n",
    "        # feature is categorical   \n",
    "        else:\n",
    "            data_below = data[split_column_values == split_value]\n",
    "            data_above = data[split_column_values != split_value]\n",
    "\n",
    "        return data_below, data_above\n",
    "    \n",
    "    # 2. Decision Tree Algorithm\n",
    "    # 2.1 Helper Function\n",
    "    def determine_type_of_feature(self, df): # 判斷變數是 連續的 還是 離散的\n",
    "        feature_types = []\n",
    "        n_unique_values_threshold = 15\n",
    "        for feature in df.columns:\n",
    "            if feature != 'is_claim':\n",
    "                unique_values = df[feature].unique()\n",
    "                example_value = unique_values[0]\n",
    "\n",
    "                if (isinstance(example_value, str)) or (len(unique_values) <= n_unique_values_threshold):\n",
    "                    feature_types.append(\"categorical\")\n",
    "                else:\n",
    "                    feature_types.append(\"continuous\")\n",
    "\n",
    "        return feature_types\n",
    "\n",
    "    # 2.2 Algorithm\n",
    "    def decision_tree_algorithm(self, df, counter=0, min_samples=2, max_depth=5):\n",
    "\n",
    "        # data preparations\n",
    "        if counter == 0:\n",
    "            global COLUMN_HEADERS, FEATURE_TYPES\n",
    "            COLUMN_HEADERS = df.columns\n",
    "            FEATURE_TYPES = self.determine_type_of_feature(df)\n",
    "            data = df.values\n",
    "        else:\n",
    "            data = df           \n",
    "\n",
    "        # base cases\n",
    "        if (self.check_purity(data)) or (len(data) < min_samples) or (counter == max_depth):\n",
    "            leaf = self.create_leaf(data)\n",
    "            return leaf\n",
    "\n",
    "\n",
    "        # recursive part\n",
    "        else:    \n",
    "            counter += 1\n",
    "\n",
    "            # helper functions \n",
    "            potential_splits = self.get_potential_split(data)\n",
    "            split_column, split_value = self.determine_best_split(data, potential_splits)\n",
    "            data_below, data_above = self.split_data(data, split_column, split_value)\n",
    "\n",
    "            # check for empty data\n",
    "            if len(data_below) == 0 or len(data_above) == 0:\n",
    "                leaf = self.create_leaf(data)\n",
    "                return leaf\n",
    "\n",
    "            # determine question\n",
    "            feature_name = COLUMN_HEADERS[split_column]\n",
    "            type_of_feature = FEATURE_TYPES[split_column]\n",
    "            if type_of_feature == \"continuous\":\n",
    "                question = \"{} <= {}\".format(feature_name, split_value)\n",
    "\n",
    "            # feature is categorical\n",
    "            else:\n",
    "                question = \"{} = {}\".format(feature_name, split_value)\n",
    "\n",
    "            # instantiate sub-tree\n",
    "            sub_tree = {question: []}\n",
    "\n",
    "            # find answers (recursion)\n",
    "            yes_answer = self.decision_tree_algorithm(data_below, counter, min_samples, max_depth)\n",
    "            no_answer = self.decision_tree_algorithm(data_above, counter, min_samples, max_depth)\n",
    "\n",
    "            # If the answers are the same, then there is no point in asking the qestion.\n",
    "            # This could happen when the data is classified even though it is not pure\n",
    "            # yet (min_samples or max_depth base case).\n",
    "            if yes_answer == no_answer:\n",
    "                sub_tree = yes_answer\n",
    "            else:\n",
    "                sub_tree[question].append(yes_answer)\n",
    "                sub_tree[question].append(no_answer)\n",
    "\n",
    "            return sub_tree\n",
    "\n",
    "    # 3. Make predictions\n",
    "    # 3.1 One example\n",
    "    def predict_example(self, example, tree):\n",
    "\n",
    "        # tree is just a root node\n",
    "        if not isinstance(tree, dict):\n",
    "            return tree\n",
    "\n",
    "        question = list(tree.keys())[0]\n",
    "        feature_name, comparison_operator, value = question.split(\" \")\n",
    "\n",
    "        # ask question\n",
    "        if comparison_operator == \"<=\":\n",
    "            if example[feature_name] <= float(value):\n",
    "                answer = tree[question][0]\n",
    "            else:\n",
    "                answer = tree[question][1]\n",
    "\n",
    "        # feature is categorical\n",
    "        else:\n",
    "            if str(example[feature_name]) == value:\n",
    "                answer = tree[question][0]\n",
    "            else:\n",
    "                answer = tree[question][1]\n",
    "\n",
    "        # base case\n",
    "        if not isinstance(answer, dict):\n",
    "            return answer\n",
    "\n",
    "        # recursive part\n",
    "        else:\n",
    "            residual_tree = answer\n",
    "            return self.predict_example(example, residual_tree)\n",
    "\n",
    "\n",
    "    # 3.2 All examples of a dataframe\n",
    "    def make_predictions(self, df, tree):\n",
    "        predictions = df.apply(self.predict_example, args=(tree,), axis=1)\n",
    "        return predictions\n",
    "\n",
    "    def bootstrap(self, data, n) : \n",
    "        index = np.random.randint(0, len(data), n)\n",
    "        df = data.iloc[index, ::]\n",
    "        \n",
    "        return df\n",
    "\n",
    "    def random_forest_algorithm(self, train, n_trees, n_bootstrap, n_features, dt_max_depth) :\n",
    "        forest = []\n",
    "        for i in range(n_trees) : \n",
    "            df_bootstrap = self.bootstrap(train, n_bootstrap)\n",
    "            tree = self.decision_tree_algorithm(df_bootstrap, max_depth=dt_max_depth)\n",
    "            forest.append(tree)\n",
    "            \n",
    "        return forest\n",
    "    \n",
    "    def random_forest_predictions(self, test, forest) :\n",
    "        pred = {}\n",
    "        for i in range(len(forest)) : \n",
    "            column = 'tree{}'.format(i)\n",
    "            p = self.make_predictions(test, tree=forest[i])\n",
    "            pred[column] = p\n",
    "            \n",
    "        pred = pd.DataFrame(pred)\n",
    "        rf = pred.mode(axis = 1)[0]\n",
    "        \n",
    "        return rf\n",
    "\n",
    "'''        \n",
    "        \n",
    "    # 3.3 Accuracy\n",
    "    def calculate_accuracy(self, df, tree):\n",
    "        predictions = make_predictions(df, tree)\n",
    "        predictions_correct = predictions == df.label\n",
    "        accuracy = predictions_correct.mean()\n",
    "\n",
    "        return accuracy\n",
    "''' \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "dfbdf912-b286-4031-9a79-d6825711f59c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250.83877420425415\n"
     ]
    }
   ],
   "source": [
    "s = time.time()\n",
    "train2 = train1\n",
    "a = round(len(train2) * 0.7)\n",
    "random.seed(123)\n",
    "w = random.sample(range(len(train2)), a)\n",
    "tr = train2.iloc[w, ::]\n",
    "te = train2.drop(w)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "r = rf.random_forest_algorithm(tr, n_trees = 100, n_bootstrap = 3000, n_features = 5, dt_max_depth = 10)\n",
    "result = rf.random_forest_predictions(te, r )\n",
    "    \n",
    "print(time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "09291f38-5a63-4ced-a05c-c0094b679e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17578\n",
      "17578\n",
      "0\n",
      "0.9342359767891683\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "print(len(te))\n",
    "print(len(result))\n",
    "print(sum(result))\n",
    "acc = accuracy_score(result, te.iloc[::, -1]) \n",
    "print(acc)\n",
    "print(sum(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6529d11-0c22-43a8-af3f-5843f2e54161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f2351f3-f423-4c0d-b49f-b528b2deed26",
   "metadata": {},
   "source": [
    "## Random Forest Classifier (scikit-learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "46fd484f-cc76-4733-958e-48ba5c328bb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9199567641369895\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train.iloc[::, :-1], train.iloc[::, -1], test_size = 0.3)\n",
    "\n",
    "# 建立 random forest 模型\n",
    "forest = ensemble.RandomForestClassifier(n_estimators = 100)\n",
    "forest_fit = forest.fit(X_train, y_train)\n",
    "# 預測\n",
    "test_y_predicted = forest.predict(X_test)\n",
    "# 績效\n",
    "accuracy = metrics.accuracy_score(y_test, test_y_predicted)\n",
    "print('Random Forest Classifier Package Accuracy : ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ee6853-4776-4122-827c-da8df7590499",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "ac1a5af5-aec9-4536-a7e5-2a07e2e56f47",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集:  0.9351440971375627\n",
      "測試集:  0.9381044487427466\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train.iloc[::, :-1], train.iloc[::, -1], test_size = 0.3)\n",
    "# 建立 XGBClassifier 模型\n",
    "xgboostModel = XGBClassifier() # , enable_categorical = True\n",
    "# 使用訓練資料訓練模型\n",
    "xgboostModel.fit(X_train, y_train)\n",
    "# 使用訓練資料預測分類\n",
    "predicted = xgboostModel.predict(X_train)\n",
    "# 預測成功的比例\n",
    "print('訓練集: ',xgboostModel.score(X_train,y_train))\n",
    "print('測試集: ',xgboostModel.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084cd432-83e6-4d3c-a976-6f0094512d52",
   "metadata": {},
   "source": [
    "## CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 459,
   "id": "c854aef2-b65b-417e-9c86-d991ede8123b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.057155\n",
      "0:\tlearn: 0.9370665\ttest: 0.9351536\tbest: 0.9351536 (0)\ttotal: 27.4ms\tremaining: 54.7s\n",
      "500:\tlearn: 0.9371749\ttest: 0.9341784\tbest: 0.9351536 (0)\ttotal: 9.13s\tremaining: 27.3s\n",
      "1000:\tlearn: 0.9372020\ttest: 0.9336909\tbest: 0.9351536 (0)\ttotal: 18.5s\tremaining: 18.4s\n",
      "1500:\tlearn: 0.9372020\ttest: 0.9336909\tbest: 0.9351536 (0)\ttotal: 27.6s\tremaining: 9.19s\n",
      "1999:\tlearn: 0.9372020\ttest: 0.9336909\tbest: 0.9351536 (0)\ttotal: 36.3s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.9351535836\n",
      "bestIteration = 0\n",
      "\n",
      "Shrink model to first 1 iterations.\n",
      "CatBoost Package Accuracy :  0.9340653089088633\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train1.iloc[::, :-1], train1.iloc[::, -1], test_size = 0.3)\n",
    "X_train, X_eval, y_train, y_eval = train_test_split(X_train, y_train, test_size = 0.1)\n",
    "clf = CatBoostClassifier(iterations=2000, eval_metric = 'Accuracy', verbose = 500)\n",
    "# Fit model\n",
    "clf.fit(X_train, y_train, eval_set = (X_eval, y_eval))\n",
    "# Make predictions\n",
    "pred = clf.predict(X_test)\n",
    "acc = accuracy_score(pred, y_test) \n",
    "print('CatBoost Package Accuracy : ', acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be40fc94-fe8c-44d1-a6d4-f014b3a31de9",
   "metadata": {},
   "source": [
    "## LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "9a2f24bf-70c7-4684-803f-c9871e8b114e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[16460     0]\n",
      " [ 1118     0]]\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train1.iloc[::, :-1], train1.iloc[::, -1], test_size = 0.3)\n",
    "classifier = lgb.LGBMClassifier(objective = 'binary', \n",
    "                                learning_rate = 0.05, \n",
    "                                n_estimators = 100, \n",
    "                                random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "fbc20929-8fc4-4fea-819f-4d52305ab02b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KFold(n_splits=3, random_state=None, shuffle=False)\n",
      "TRAIN: [2 3] TEST: [0 1]\n",
      "TRAIN: [0 1 3] TEST: [2]\n",
      "TRAIN: [0 1 2] TEST: [3]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[1, 2], [3, 4], [1, 2], [3, 4]])\n",
    "y = np.array([1, 2, 3, 4])\n",
    "kf = KFold(n_splits=3)\n",
    "kf.get_n_splits(X)\n",
    "print(kf)\n",
    "for train_index, test_index in kf.split(X):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = X[train_index], X[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0ebb93-dbed-4c13-bd6b-f5baa326a760",
   "metadata": {},
   "source": [
    "##　Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "c741af4e-ec52-409c-89e0-25abb86c310f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'unique'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [514], line 30\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# self naive bayes\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# a = Naive_Bayes()\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# a.fit(X_train, y_train)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     27\u001b[0m \n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# self random forest\u001b[39;00m\n\u001b[1;32m     29\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestClassifier()\n\u001b[0;32m---> 30\u001b[0m r \u001b[38;5;241m=\u001b[39m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_forest_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trees\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_bootstrap\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_features\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdt_max_depth\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m result \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mrandom_forest_predictions(pd\u001b[38;5;241m.\u001b[39mconcat([X_test, y_test], axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m), r )\n\u001b[1;32m     33\u001b[0m ranfor\u001b[38;5;241m.\u001b[39mappend(result)\n",
      "Cell \u001b[0;32mIn [373], line 237\u001b[0m, in \u001b[0;36mRandomForestClassifier.random_forest_algorithm\u001b[0;34m(self, train, n_trees, n_bootstrap, n_features, dt_max_depth)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_trees) : \n\u001b[1;32m    236\u001b[0m     df_bootstrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbootstrap(train, n_bootstrap)\n\u001b[0;32m--> 237\u001b[0m     tree \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecision_tree_algorithm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_bootstrap\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_depth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdt_max_depth\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m     forest\u001b[38;5;241m.\u001b[39mappend(tree)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forest\n",
      "Cell \u001b[0;32mIn [373], line 134\u001b[0m, in \u001b[0;36mRandomForestClassifier.decision_tree_algorithm\u001b[0;34m(self, df, counter, min_samples, max_depth)\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mglobal\u001b[39;00m COLUMN_HEADERS, FEATURE_TYPES\n\u001b[1;32m    133\u001b[0m     COLUMN_HEADERS \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mcolumns\n\u001b[0;32m--> 134\u001b[0m     FEATURE_TYPES \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetermine_type_of_feature\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     data \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "Cell \u001b[0;32mIn [373], line 117\u001b[0m, in \u001b[0;36mRandomForestClassifier.determine_type_of_feature\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m feature \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mis_claim\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 117\u001b[0m         unique_values \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m()\n\u001b[1;32m    118\u001b[0m         example_value \u001b[38;5;241m=\u001b[39m unique_values[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(example_value, \u001b[38;5;28mstr\u001b[39m)) \u001b[38;5;129;01mor\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(unique_values) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m n_unique_values_threshold):\n",
      "File \u001b[0;32m~/miniconda3/envs/jupyterlab/lib/python3.9/site-packages/pandas/core/generic.py:5902\u001b[0m, in \u001b[0;36mNDFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5895\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   5896\u001b[0m     name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_internal_names_set\n\u001b[1;32m   5897\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_metadata\n\u001b[1;32m   5898\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_accessors\n\u001b[1;32m   5899\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_info_axis\u001b[38;5;241m.\u001b[39m_can_hold_identifiers_and_holds_name(name)\n\u001b[1;32m   5900\u001b[0m ):\n\u001b[1;32m   5901\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m[name]\n\u001b[0;32m-> 5902\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mobject\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__getattribute__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'unique'"
     ]
    }
   ],
   "source": [
    "k = [3, 5, 10]\n",
    "for i in k : \n",
    "    # k fold\n",
    "    random.seed(1)\n",
    "    kf = KFold(n_splits=i)\n",
    "    t = np.array(train1)\n",
    "    kf.get_n_splits(t[::, :-1])\n",
    "    nb = []\n",
    "    ranfor = []\n",
    "    for train_index, test_index in kf.split(t[::, :-1]):\n",
    "        \n",
    "        X_train, X_test = t[::, :-1][train_index], t[::, :-1][test_index]\n",
    "        y_train, y_test = t[::, -1][train_index], t[::, -1][test_index]\n",
    "        \n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_test = pd.DataFrame(X_test)\n",
    "        y_train = pd.DataFrame(y_train)\n",
    "        y_test = pd.DataFrame(y_test)\n",
    "        \n",
    "        \n",
    "        # self naive bayes\n",
    "        # a = Naive_Bayes()\n",
    "        # a.fit(X_train, y_train)\n",
    "        # p = a.predict(X_test)\n",
    "        # acc = accuracy_score(p, np.array(y_test)) \n",
    "        # nb.append(acc)\n",
    "        \n",
    "        # self random forest\n",
    "        rf = RandomForestClassifier()\n",
    "        r = rf.random_forest_algorithm(pd.concat([X_train, y_train], axis = 1), n_trees = 100, n_bootstrap = 3000, n_features = 5, dt_max_depth = 10)\n",
    "        result = rf.random_forest_predictions(pd.concat([X_test, y_test], axis = 1), r )\n",
    "        \n",
    "        ranfor.append(result)\n",
    "    print('Self Random Forest ', i, ' fold cross validation : ', ranfor)\n",
    "    print('Self Random Forest ', i, ' fold cross validation : ', ranfor.mean())\n",
    "    \n",
    "        # self random forest\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "d2df0af5-f450-49de-b254-f993b3e123b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "250.83877420425415\n"
     ]
    }
   ],
   "source": [
    "# self rf 250s\n",
    "s = time.time()\n",
    "a = round(len(train1) * 0.7)\n",
    "random.seed(123)\n",
    "w = random.sample(range(len(train1)), a)\n",
    "tr = train1.iloc[w, ::]\n",
    "te = train1.drop(w)\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "r = rf.random_forest_algorithm(tr, n_trees = 100, n_bootstrap = 3000, n_features = 5, dt_max_depth = 10)\n",
    "result = rf.random_forest_predictions(te, r )\n",
    "    \n",
    "print(time.time() - s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e1f5c7-386a-4a78-b7c5-2d14129f3b40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "8a25cfa5-f747-41f3-ace1-a10f71d9cb35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest  3  fold cross validation :  [0.93558958 0.93528237 0.93584229]\n",
      "RandomForest  3  fold cross validation :  0.9355714136892446\n",
      "XGboost  3  fold cross validation :  [0.93558958 0.93569198 0.9359959 ]\n",
      "XGboost  3  fold cross validation :  0.9357591520472367\n",
      "Learning rate set to 0.026095\n",
      "0:\tlearn: 0.9360231\ttotal: 34.7ms\tremaining: 1m 9s\n",
      "500:\tlearn: 0.9360231\ttotal: 8.33s\tremaining: 24.9s\n",
      "1000:\tlearn: 0.9361255\ttotal: 17.3s\tremaining: 17.3s\n",
      "1500:\tlearn: 0.9361255\ttotal: 26.3s\tremaining: 8.75s\n",
      "1999:\tlearn: 0.9361255\ttotal: 35.2s\tremaining: 0us\n",
      "Learning rate set to 0.026095\n",
      "0:\tlearn: 0.9360487\ttotal: 25.7ms\tremaining: 51.4s\n",
      "500:\tlearn: 0.9360487\ttotal: 8.13s\tremaining: 24.3s\n",
      "1000:\tlearn: 0.9361255\ttotal: 17.2s\tremaining: 17.2s\n",
      "1500:\tlearn: 0.9361255\ttotal: 26.3s\tremaining: 8.74s\n",
      "1999:\tlearn: 0.9361255\ttotal: 35.7s\tremaining: 0us\n",
      "Learning rate set to 0.026096\n",
      "0:\tlearn: 0.9360248\ttotal: 25.4ms\tremaining: 50.7s\n",
      "500:\tlearn: 0.9360248\ttotal: 8.15s\tremaining: 24.4s\n",
      "1000:\tlearn: 0.9360760\ttotal: 17.3s\tremaining: 17.2s\n",
      "1500:\tlearn: 0.9361016\ttotal: 26.4s\tremaining: 8.78s\n",
      "1999:\tlearn: 0.9361272\ttotal: 35.5s\tremaining: 0us\n",
      "CatBoost  3  fold cross validation :  [0.93538477 0.93558958 0.93584229]\n",
      "CatBoost  3  fold cross validation :  0.9356055474594901\n",
      "LightGBM  3  fold cross validation :  [0.93605038 0.93599918 0.93604711]\n",
      "LightGBM  3  fold cross validation :  0.9360322230830818\n",
      "RandomForest  5  fold cross validation :  [0.93574537 0.93557471 0.93573989 0.93591056 0.93591056]\n",
      "RandomForest  5  fold cross validation :  0.9357762191486856\n",
      "XGboost  5  fold cross validation :  [0.93574537 0.93591603 0.9359959  0.93608124 0.93591056]\n",
      "XGboost  5  fold cross validation :  0.935929823154044\n",
      "Learning rate set to 0.028208\n",
      "0:\tlearn: 0.9360399\ttotal: 27.9ms\tremaining: 55.8s\n",
      "500:\tlearn: 0.9360399\ttotal: 8.72s\tremaining: 26.1s\n",
      "1000:\tlearn: 0.9361039\ttotal: 18.2s\tremaining: 18.1s\n",
      "1500:\tlearn: 0.9361039\ttotal: 27.6s\tremaining: 9.19s\n",
      "1999:\tlearn: 0.9361039\ttotal: 37.1s\tremaining: 0us\n",
      "Learning rate set to 0.028208\n",
      "0:\tlearn: 0.9360399\ttotal: 27.5ms\tremaining: 54.9s\n",
      "500:\tlearn: 0.9360399\ttotal: 8.67s\tremaining: 25.9s\n",
      "1000:\tlearn: 0.9360613\ttotal: 18.1s\tremaining: 18.1s\n",
      "1500:\tlearn: 0.9360613\ttotal: 27.6s\tremaining: 9.18s\n",
      "1999:\tlearn: 0.9360613\ttotal: 37s\tremaining: 0us\n",
      "Learning rate set to 0.028208\n",
      "0:\tlearn: 0.9360200\ttotal: 23.2ms\tremaining: 46.4s\n",
      "500:\tlearn: 0.9360200\ttotal: 8.52s\tremaining: 25.5s\n",
      "1000:\tlearn: 0.9360626\ttotal: 17.9s\tremaining: 17.8s\n",
      "1500:\tlearn: 0.9360626\ttotal: 27.3s\tremaining: 9.08s\n",
      "1999:\tlearn: 0.9360626\ttotal: 36.8s\tremaining: 0us\n",
      "Learning rate set to 0.028208\n",
      "0:\tlearn: 0.9360200\ttotal: 16.4ms\tremaining: 32.8s\n",
      "500:\tlearn: 0.9360200\ttotal: 8.45s\tremaining: 25.3s\n",
      "1000:\tlearn: 0.9360413\ttotal: 18s\tremaining: 17.9s\n",
      "1500:\tlearn: 0.9360840\ttotal: 27.4s\tremaining: 9.1s\n",
      "1999:\tlearn: 0.9360840\ttotal: 36.7s\tremaining: 0us\n",
      "Learning rate set to 0.028208\n",
      "0:\tlearn: 0.9360413\ttotal: 20.1ms\tremaining: 40.1s\n",
      "500:\tlearn: 0.9360840\ttotal: 8.4s\tremaining: 25.1s\n",
      "1000:\tlearn: 0.9360840\ttotal: 17.6s\tremaining: 17.6s\n",
      "1500:\tlearn: 0.9360840\ttotal: 26.9s\tremaining: 8.94s\n",
      "1999:\tlearn: 0.9360840\ttotal: 36.2s\tremaining: 0us\n",
      "CatBoost  5  fold cross validation :  [0.93574537 0.93591603 0.93582523 0.93591056 0.93591056]\n",
      "CatBoost  5  fold cross validation :  0.9358615521180311\n",
      "LightGBM  5  fold cross validation :  [0.93600137 0.93600137 0.93608124 0.93608124 0.9359959 ]\n",
      "LightGBM  5  fold cross validation :  0.9360322238823928\n",
      "RandomForest  10  fold cross validation :  [0.93600683 0.93583618 0.9359959  0.93616658 0.9359959  0.93531319\n",
      " 0.93582523 0.93582523 0.93565455 0.93582523]\n",
      "RandomForest  10  fold cross validation :  0.9358444812595424\n",
      "XGboost  10  fold cross validation :  [0.93600683 0.93600683 0.9359959  0.93616658 0.9359959  0.93582523\n",
      " 0.9359959  0.9359959  0.93565455 0.93582523]\n",
      "XGboost  10  fold cross validation :  0.9359468849009751\n",
      "Learning rate set to 0.029663\n",
      "0:\tlearn: 0.9360350\ttotal: 33.7ms\tremaining: 1m 7s\n",
      "500:\tlearn: 0.9360350\ttotal: 8.83s\tremaining: 26.4s\n",
      "1000:\tlearn: 0.9360540\ttotal: 18.2s\tremaining: 18.2s\n",
      "1500:\tlearn: 0.9360540\ttotal: 27.7s\tremaining: 9.21s\n",
      "1999:\tlearn: 0.9360540\ttotal: 37s\tremaining: 0us\n",
      "Learning rate set to 0.029663\n",
      "0:\tlearn: 0.9360350\ttotal: 31.3ms\tremaining: 1m 2s\n",
      "500:\tlearn: 0.9360350\ttotal: 8.65s\tremaining: 25.9s\n",
      "1000:\tlearn: 0.9360730\ttotal: 18.1s\tremaining: 18.1s\n",
      "1500:\tlearn: 0.9360730\ttotal: 27.5s\tremaining: 9.13s\n",
      "1999:\tlearn: 0.9360730\ttotal: 36.7s\tremaining: 0us\n",
      "Learning rate set to 0.029663\n",
      "0:\tlearn: 0.9360173\ttotal: 23.5ms\tremaining: 47s\n",
      "500:\tlearn: 0.9360173\ttotal: 8.66s\tremaining: 25.9s\n",
      "1000:\tlearn: 0.9360363\ttotal: 18.1s\tremaining: 18.1s\n",
      "1500:\tlearn: 0.9360363\ttotal: 27.4s\tremaining: 9.12s\n",
      "1999:\tlearn: 0.9360363\ttotal: 36.7s\tremaining: 0us\n",
      "Learning rate set to 0.029663\n",
      "0:\tlearn: 0.9360173\ttotal: 24.1ms\tremaining: 48.1s\n",
      "500:\tlearn: 0.9360173\ttotal: 8.63s\tremaining: 25.8s\n",
      "1000:\tlearn: 0.9360363\ttotal: 18.1s\tremaining: 18.1s\n",
      "1500:\tlearn: 0.9360363\ttotal: 27.4s\tremaining: 9.11s\n",
      "1999:\tlearn: 0.9360363\ttotal: 36.7s\tremaining: 0us\n",
      "Learning rate set to 0.029663\n",
      "0:\tlearn: 0.9360363\ttotal: 23.1ms\tremaining: 46.2s\n",
      "500:\tlearn: 0.9360363\ttotal: 8.51s\tremaining: 25.5s\n",
      "1000:\tlearn: 0.9360363\ttotal: 17.9s\tremaining: 17.8s\n",
      "1500:\tlearn: 0.9360363\ttotal: 27.3s\tremaining: 9.08s\n",
      "1999:\tlearn: 0.9360363\ttotal: 36.6s\tremaining: 0us\n",
      "Learning rate set to 0.029663\n",
      "0:\tlearn: 0.9360363\ttotal: 26.6ms\tremaining: 53.2s\n",
      "500:\tlearn: 0.9360363\ttotal: 8.54s\tremaining: 25.6s\n",
      "1000:\tlearn: 0.9360742\ttotal: 18s\tremaining: 18s\n",
      "1500:\tlearn: 0.9360931\ttotal: 27.5s\tremaining: 9.16s\n",
      "1999:\tlearn: 0.9360931\ttotal: 36.9s\tremaining: 0us\n",
      "Learning rate set to 0.029663\n",
      "0:\tlearn: 0.9360363\ttotal: 20.6ms\tremaining: 41.1s\n",
      "500:\tlearn: 0.9360363\ttotal: 8.49s\tremaining: 25.4s\n",
      "1000:\tlearn: 0.9360552\ttotal: 18s\tremaining: 18s\n",
      "1500:\tlearn: 0.9360742\ttotal: 27.5s\tremaining: 9.13s\n",
      "1999:\tlearn: 0.9360742\ttotal: 36.8s\tremaining: 0us\n",
      "Learning rate set to 0.029663\n",
      "0:\tlearn: 0.9360363\ttotal: 21.4ms\tremaining: 42.9s\n",
      "500:\tlearn: 0.9360363\ttotal: 8.68s\tremaining: 26s\n",
      "1000:\tlearn: 0.9360552\ttotal: 18.1s\tremaining: 18.1s\n",
      "1500:\tlearn: 0.9360742\ttotal: 27.7s\tremaining: 9.2s\n",
      "1999:\tlearn: 0.9360742\ttotal: 37.1s\tremaining: 0us\n",
      "Learning rate set to 0.029663\n",
      "0:\tlearn: 0.9360363\ttotal: 30.7ms\tremaining: 1m 1s\n",
      "500:\tlearn: 0.9360363\ttotal: 8.57s\tremaining: 25.6s\n",
      "1000:\tlearn: 0.9360742\ttotal: 18s\tremaining: 18s\n",
      "1500:\tlearn: 0.9360742\ttotal: 27.4s\tremaining: 9.11s\n",
      "1999:\tlearn: 0.9360742\ttotal: 36.7s\tremaining: 0us\n",
      "Learning rate set to 0.029663\n",
      "0:\tlearn: 0.9360363\ttotal: 20.6ms\tremaining: 41.2s\n",
      "500:\tlearn: 0.9360742\ttotal: 8.55s\tremaining: 25.6s\n",
      "1000:\tlearn: 0.9360742\ttotal: 17.9s\tremaining: 17.9s\n",
      "1500:\tlearn: 0.9360742\ttotal: 27.3s\tremaining: 9.09s\n",
      "1999:\tlearn: 0.9360742\ttotal: 36.7s\tremaining: 0us\n",
      "CatBoost  10  fold cross validation :  [0.93600683 0.93583618 0.9359959  0.93616658 0.9359959  0.93548387\n",
      " 0.93582523 0.93582523 0.93565455 0.93582523]\n",
      "CatBoost  10  fold cross validation :  0.9358615490185457\n",
      "LightGBM  10  fold cross validation :  [0.93600683 0.93600683 0.93616658 0.93616658 0.9359959  0.9359959\n",
      " 0.9359959  0.9359959  0.9359959  0.9359959 ]\n",
      "LightGBM  10  fold cross validation :  0.9360322236959912\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(train1.iloc[::, :-1], train1.iloc[::, -1], test_size = 0.3)\n",
    "k = [3, 5, 10]\n",
    "for i in k : \n",
    "    # RandomForest\n",
    "    scores = cross_val_score(ensemble.RandomForestClassifier(n_estimators = 100),train1.iloc[::, :-1], train1.iloc[::, -1], cv = i, scoring = 'accuracy')\n",
    "    print('RandomForest ', i, ' fold cross validation : ', scores)\n",
    "    print('RandomForest ', i, ' fold cross validation : ', scores.mean())\n",
    "    \n",
    "    # XGBoost\n",
    "    scores = cross_val_score(XGBClassifier(),train1.iloc[::, :-1], train1.iloc[::, -1], cv = i, scoring = 'accuracy')\n",
    "    print('XGboost ', i, ' fold cross validation : ', scores)\n",
    "    print('XGboost ', i, ' fold cross validation : ', scores.mean())\n",
    "    \n",
    "    #Catboost\n",
    "    scores = cross_val_score(CatBoostClassifier(iterations=2000, eval_metric = 'Accuracy', verbose = 500),train1.iloc[::, :-1], train1.iloc[::, -1], cv = i, scoring = 'accuracy')\n",
    "    print('CatBoost ', i, ' fold cross validation : ', scores)\n",
    "    print('CatBoost ', i, ' fold cross validation : ', scores.mean())\n",
    "    \n",
    "    # LightGBM\n",
    "    scores = cross_val_score(lgb.LGBMClassifier(objective = 'binary', learning_rate = 0.05,  n_estimators = 100, random_state=0),train1.iloc[::, :-1], train1.iloc[::, -1], cv = i, scoring = 'accuracy')\n",
    "    print('LightGBM ', i, ' fold cross validation : ', scores)\n",
    "    print('LightGBM ', i, ' fold cross validation : ', scores.mean())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9d1cf1-ea3e-4601-bc77-0a77150128b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300087cc-5e09-4f20-8d6a-d3f0eeacd147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97306f27-418b-4b85-90a8-a8dc22ae38ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab",
   "language": "python",
   "name": "jupyterlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
